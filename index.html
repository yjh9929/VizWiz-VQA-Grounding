<!doctype html>

<html lang="en">
<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Lightweight Grounding Model Combining a CLIP-based Encoder With an
Upsampling Decoder</title>

  <!-- Bootstrap CSS -->
  <link href="css/animate.css" rel="stylesheet">
  <link rel="stylesheet" href="css/bootstrap.min.css" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
<div class="container-fluid pl-0 pr-0 bg-img clearfix parallax-window2" data-parallax="scroll" data-image-src="images/banner2.png">
  <nav class="navbar navbar-expand-md navbar-dark">
    <div class="container"> 
      <!-- Brand --> 
      <a class="navbar-brand mr-auto" href="#"><img src="images/logo.png" alt="Vizwiz" /></a> 
      
      <!-- Toggler/collapsibe Button -->
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsibleNavbar"> <span class="navbar-toggler-icon"></span> </button>
      
      <!-- Navbar links -->
      <div class="collapse navbar-collapse" id="collapsibleNavbar">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item"> <a class="nav-link" href="#">Home</a> </li>
          <li class="nav-item"> <a class="nav-link" href="#about-us">TASKS</a> </li>
          <li class="nav-item"> <a class="nav-link" href="#methods">METHODS</a> </li>
          <li class="nav-item"> <a class="nav-link" href="#contact">Contact</a> </li>
        </ul>
        <ul class="navbar-nav ml-5">
          <li class="nav-item"> <a class="nav-link btn btn-danger" href="#">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</a> </li>
        </ul>
      </div>
    </div>
  </nav>
  <div class="container">
    <div class="fh5co-banner-text-box">
      <div class="quote-box pl-5 pr-5 wow bounceInRight">
        <h2> Lightweight Grounding Model Combining a CLIP-based Encoder With an
Upsampling Decoder <br><span>BY BANDI</span> </h2>
      </div>
       </div>
  </div>
</div>
<div id="about-us" class="container-fluid fh5co-network">

  <div class="container">
    <div class="row">
      <div class="col-md-6">
        <h4 class="wow bounceInUp"></h4>
        <h2 class="wow bounceInRight">TASKS</h2>
        <hr />
        <h5 class="wow bounceInLeft">VizWiz-VQA-Grounding</h5>
        <p class="wow bounceInDown">Given a visual question (question-image pair), the task is to return the region in the image used to arrive at the answer to the visual question. The submissions will be evaluated based on the mean Intersection over Union (IoU) score across all test images. The team which achieves the maximum IoU score wins this challenge.</p>
      </div>
      <div class="col-md-6">
        <figure class="wow bounceInDown"> <img src="images/m2.png" alt="gym" class="img-fluid" /> </figure>
      </div>
    </div>
  </div>
</div>

<div id="methods" class="container-fluid fh5co-about-us pl-0 pr-0" style="background-color: #5e564e; ">
  <div class="container">
    <div class="col-sm-6 offset-sm-6">
      <h2 class="wow bounceInLeft" data-wow-delay=".25s">METHODS</h2>
      <hr/>
      <p class="wow bounceInRight" data-wow-delay=".25s">
        In this section, we provide a detailed description of the three
main components of the proposed mode
1) CLIP Encoder. We use the pre-trained CLIP ViTL/14-336 model [3] to extract embeddings from both the
image and the concatenated question-answer pair. The visual embedding is taken from the final transformer layer,
excluding the CLS token, and reshaped into a spatial feature map of shape (B, D, H, W).
2) Cross-Attention Block. A single-layer multi-head
cross-attention is employed to align textual and visual information. Specifically, the image tokens (flattened spatial
features) serve as queries, while the text tokens act as the
keys and values. This allows the model to dynamically attend to relevant image regions based on the contextual information provided by the question.
3) Lightweight U-Net Decoder. The fused features
from the cross-attention block are reshaped back into spatial form and passed through a lightweight U-Net-style decoder [4], which produces the final binary mask indicating
the answer grounding region.

      </p>
    </div>
  </div>
</div>




<div class="container-fluid fh5co-content-box">
  <div class="container">
    <div class="row">
      <div class="col-md-5 pr-0"><img src="images/model.png" alt="gym" class="img-fluid wow bounceInLeft" /> </div>
      <div class="col-md-7 pl-0">
        <div class="wow bounceInRight" data-wow-delay=".25s">
          <div class="card-img-overlay">
            <p>The cross-attention block is applied after visual features
are extracted and flattened, but before being reshaped and
decoded. This design allows the early integration of semantic cues into the spatial reasoning process. </p>
          </div>
          <div class="img-fluid" style="background-color: #fdf8f3; width: 100%; height: 300px;"></div>
        </div>
      </div>
    </div>

<div class="row gallery">
      <div class="col-md-4">
        <div class="card">
          <div class="card-body mb-4 wow bounceInLeft" data-wow-delay=".25s">
            <h4 class="card-title">Limitation 1</h4>
            <p class="card-text">ILimitation of Random Crop Preprocessing Without
Considering Dataset Characteristics. </p>
              <img class="card-img-top img-fluid wow bounceInRight" data-wow-delay=".25s" src="images/m1.png" alt="Card image">
              </div>
           </div>
      </div>

      <div class="col-md-4">
        <div class="card"> 
          <div class="card-body mt-4 wow bounceInDown" data-wow-delay=".25s">
            <h4 class="card-title">Limitation 2</h4>
            <p class="card-text">Difficulty in Detecting Small and Edge-Located Objects. </p>
            <img class="card-img-top img-fluid wow bounceInUp" data-wow-delay=".25s" src="images/m3.png" alt="Card image">
          </div>
        </div>
      </div>

      <div class="col-md-4">
        <div class="card">
          <div class="card-body mb-4 wow bounceInRight" data-wow-delay=".25s">
            <h4 class="card-title">Limitation 3</h4>
            <p class="card-text">Limitation in Binary Conversion of Grounding Regions </p>
            <img class="card-img-top img-fluid wow bounceInLeft" data-wow-delay=".25s" src="images/task.png" alt="Card image">
            </div>
          </div>
      </div>
    </div>


    <div class="row trainers pl-0 pr-0">
      <div class="col-12 bg-50">
        <div class="quote-box2 wow bounceInDown" data-wow-delay=".25s">
          <h2> MEMBERS</h2>
      </div>
    </div>

      <!-- 윤지희 -->
      <div class="col-md-4 pr-4 pl-4">
        <div class="card text-center wow bounceInLeft" data-wow-delay=".25s">
          <img class="card-img-top rounded-circle img-fluid" src="images/jihee2.jpg" alt="Card image" style="width: 100%; height: auto; object-fit: cover;"
        class="card-img-top img-fluid">
          <div class="card-body mb-5">
            <h4 class="card-title">JIHEE YOON</h4>
            <p class="card-text">I’m a master’s student in AI at the CVML Lab, Chung-Ang University. My research focuses at the intersection of vision, language, and generative AI.
    In this project, I took the lead on designing the model structure, analyzing failure cases, and driving the overall direction of the study — especially in articulating its limitations and what we learned from them.</p>
          </div>
        </div>
      </div>

      <!-- 이승아 -->
      <div class="col-md-4 pr-4 pl-4">
        <div class="card text-center wow bounceInUp" data-wow-delay=".25s">
          <img class="card-img-top rounded-circle img-fluid" src="images/seunga.jpg" alt="Card image" style="width: 100%; height: auto; object-fit: cover;"
        class="card-img-top img-fluid">
          <div class="card-body mb-5">
            <h4 class="card-title">SEUNGA LEE</h4>
            <p class="card-text">I am currently studying in the Department of Computer Science and Engineering at Chung-Ang University, with interest in computer vision and backend engineering. In this project, I was responsible for data preprocessing and visualization, and also participated in experiments to improve model performance.</p>
          </div>
        </div>
      </div>

      <!-- 정해솔 -->
      <div class="col-md-4 pr-4 pl-4">
        <div class="card text-center wow bounceInRight" data-wow-delay=".25s">
          <img class="card-img-top rounded-circle img-fluid" src="images/haesol.jpg" alt="Card image" style="width: 100%; height: auto; object-fit: cover;"
        class="card-img-top img-fluid">
          <div class="card-body mb-5">
            <h4 class="card-title">HAESOL JEONG</h4>
            <p class="card-text">I'm currently majoring in Computer Science at Chung-Ang University. I'm interested in computer vision.
              I contributed to improving model performance by modifying the decoder architecture to a lightweight version.
              and took part in as an MLOps engineer, setting up an experiment version control system and a structured framework for logging intermediate results. 
      
          </div>
        </div>
      </div>
    </div>



    
  </div>
</div>



<footer id="contact" class="container-fluid bg-dark text-white py-5">

  <div class="container text-center">
    <h4 style="margin-top: 80px;">CONTACT US</h4>
    <a href="https://docs.google.com/forms/d/e/1FAIpQLSeGjnnEn89bbWVLMDOuxNVEqr0ajMSO6TNes1ft8fUDL-tvEA/viewform"
   target="_blank"
   class="btn btn-outline-light mt-3">
  Contact Us
</a>

  </div>
</footer>

<!-- Optional JavaScript --> 
<!-- jQuery first, then Bootstrap JS, ... -->

<script src="js/jquery.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/parallax.js"></script>
<script src="js/wow.js"></script>
<script src="js/main.js"></script>

</body>
</html>
